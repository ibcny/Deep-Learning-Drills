{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import scipy\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current folder is /Users/cany/Desktop/CUDA/assignment 5\n",
      "Number of total images is 87\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "cwd = os.getcwd()\n",
    "print (\"Current folder is %s\" % (cwd) )\n",
    "\n",
    "# Training set folder \n",
    "paths = {\"images/cats\", \"images/dogs\"}\n",
    "# The reshape size\n",
    "imgsize = [112, 112]\n",
    "\n",
    "# check the total number of training data\n",
    "valid_exts = [\".jpg\",\".gif\",\".png\",\".tga\", \".jpeg\"]\n",
    "imgcnt = 0\n",
    "nclass = len(paths)\n",
    "for relpath in paths:\n",
    "    fullpath = cwd + \"/\" + relpath\n",
    "    flist = os.listdir(fullpath)\n",
    "    for f in flist:\n",
    "        if os.path.splitext(f)[1].lower() not in valid_exts:\n",
    "            continue\n",
    "        fullpath = os.path.join(fullpath, f)\n",
    "        imgcnt = imgcnt + 1\n",
    "\n",
    "print (\"Number of total images is %d\" % (imgcnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a training_tensor and test_tensor each of which has \n",
    "# (n, 112, 112, 3) dimension\n",
    "n_input_width = 112\n",
    "n_input_height = 112\n",
    "n_input_channel = 3\n",
    "\n",
    "totalimg   = np.ndarray((imgcnt, imgsize[0]*imgsize[1]*3))\n",
    "totallabel = np.ndarray((imgcnt, nclass))\n",
    "imgcnt     = 0\n",
    "for i, relpath in zip(range(nclass), paths):\n",
    "    path = cwd + \"/\" + relpath\n",
    "    flist = os.listdir(path)\n",
    "    for f in flist:\n",
    "        if os.path.splitext(f)[1].lower() not in valid_exts:\n",
    "            continue\n",
    "        fullpath = os.path.join(path, f)\n",
    "        currimg  = imread(fullpath)\n",
    "        # Reshape\n",
    "        small = imresize(currimg, [imgsize[0], imgsize[1]])/255.\n",
    "        vec   = np.reshape(small, (1, -1))\n",
    "        # Save \n",
    "        totalimg[imgcnt, :] = vec\n",
    "        totallabel[imgcnt, :] = np.eye(nclass, nclass)[i]\n",
    "        imgcnt    = imgcnt + 1\n",
    "        \n",
    "# Divide total data into training and test set\n",
    "randidx  = np.random.randint(imgcnt, size=imgcnt)\n",
    "trainidx = randidx[0:int(4*imgcnt/5)]\n",
    "testidx  = randidx[int(4*imgcnt/5):imgcnt]\n",
    "\n",
    "trainimg   = totalimg[trainidx, :]\n",
    "trainlabel = totallabel[trainidx, :]\n",
    "testimg    = totalimg[testidx, :]\n",
    "testlabel  = totallabel[testidx, :]\n",
    "\n",
    "training_tensor = np.ndarray((trainimg.shape[0], 112, 112, 3))\n",
    "for i in range(trainimg.shape[0]):\n",
    "    img = trainimg[i, :]\n",
    "    img = np.reshape(img, [112, 112, 3])\n",
    "    training_tensor[i, :, :, :] = img \n",
    "    \n",
    "testing_tensor = np.ndarray((testimg.shape[0], 112, 112, 3))\n",
    "for i in range(testimg.shape[0]):\n",
    "    img = testimg[i, :]\n",
    "    img = np.reshape(img, [112, 112, 3])\n",
    "    testing_tensor[i, :, :, :] = img \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def netw(data_path, input_image):\n",
    "    layers = (\n",
    "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
    "\n",
    "        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "\n",
    "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
    "        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "\n",
    "        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
    "        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "\n",
    "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
    "        'relu5_3', 'conv5_4', 'relu5_4'\n",
    "    )\n",
    "\n",
    "    data = scipy.io.loadmat(data_path)\n",
    "    mean = data['normalization'][0][0][0]\n",
    "    mean_pixel = np.mean(mean, axis=(0, 1))\n",
    "    weights = data['layers'][0]\n",
    "\n",
    "    net = {}\n",
    "    current = input_image\n",
    "    for i, name in enumerate(layers):\n",
    "        kind = name[:4]\n",
    "        if kind == 'conv':\n",
    "            kernels, bias = weights[i][0][0][0][0]\n",
    "            # matconvnet: weights are [width, height, in_channels, out_channels]\n",
    "            # tensorflow: weights are [height, width, in_channels, out_channels]\n",
    "            kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
    "            bias = bias.reshape(-1)\n",
    "            current = _conv_layer(current, kernels, bias)\n",
    "        elif kind == 'relu':\n",
    "            current = tf.nn.relu(current)\n",
    "        elif kind == 'pool':\n",
    "            current = _pool_layer(current)\n",
    "        net[name] = current\n",
    "\n",
    "    assert len(net) == len(layers)\n",
    "    return net, mean_pixel\n",
    "\n",
    "def _conv_layer(input, weights, bias):\n",
    "    conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1),\n",
    "            padding='SAME')\n",
    "    return tf.nn.bias_add(conv, bias)\n",
    "def _pool_layer(input):\n",
    "    return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "            padding='SAME')\n",
    "def preprocess(image, mean_pixel):\n",
    "    return image - mean_pixel\n",
    "def unprocess(image, mean_pixel):\n",
    "    return image + mean_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done\n"
     ]
    }
   ],
   "source": [
    "def imread(path):\n",
    "    return scipy.misc.imread(path).astype(np.float)\n",
    "def imsave(path, img):\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    scipy.misc.imsave(path, img)\n",
    "\n",
    "VGG_PATH = cwd + \"/data\" + \"/imagenet-vgg-verydeep-19.mat\"\n",
    "\n",
    "training_tensor = np.reshape(trainimg, (-1, n_input_width, n_input_height, n_input_channel)) # n_input_width by n_input_height matrix \n",
    "testing_tensor = np.reshape(testimg, (-1, n_input_width, n_input_height, n_input_channel)) # n_input_width by n_input_height matrix \n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    image_plc_hld = tf.placeholder(tf.float32, shape=[None, n_input_width, n_input_height, n_input_channel])\n",
    "    net, mean_pixel = netw(VGG_PATH, image_plc_hld)\n",
    "    # preprocessed training/test images\n",
    "    # why must I cancel pre-processing and do it unnormalized?\n",
    "    #training_image_pre = np.squeeze(np.array([preprocess(training_tensor, mean_pixel)]))\n",
    "    #test_image_pre = np.squeeze(np.array([preprocess(testing_tensor, mean_pixel)]))\n",
    "    # training/test features\n",
    "    train_features = net['relu5_4'].eval(feed_dict={image_plc_hld: training_tensor})\n",
    "    test_features  = net['relu5_4'].eval(feed_dict={image_plc_hld: testing_tensor})\n",
    "    \n",
    "print(\"Preprocessing done\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = train_features.shape[1] * train_features.shape[2] * train_features.shape[3]  # e.g. vectorized input\n",
    "n_hidden   = 1024 # hidden layer num units (e.g. half of input units)\n",
    "n_classes  = 2  # number of neurons in the output layer \n",
    "\n",
    "# Learning Parameters\n",
    "learning_rate    = 0.0001\n",
    "no_of_iterations = 100\n",
    "batch_size       = 100\n",
    "\n",
    "# tf Graph variables\n",
    "x = tf.placeholder(\"float\", [None, n_input], name='x')\n",
    "y = tf.placeholder(\"float\", [None, n_classes], name='y')\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# vectorize training/test data\n",
    "v_train_features = np.reshape(train_features, (-1, n_input))\n",
    "v_test_features = np.reshape(test_features, (-1, n_input))\n",
    "\n",
    "# Store layers weight & bias\n",
    "stddev = 0.1 # <== This greatly affects accuracy!! \n",
    "weights = {\n",
    "    'h_1': tf.Variable(tf.random_normal([n_input, n_hidden], stddev=stddev)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], stddev=stddev))\n",
    "}\n",
    "biases = {\n",
    "    'b_1': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Create model\n",
    "hidden_layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['h_1']), biases['b_1']))\n",
    "hidden_layer_1_drop = tf.nn.dropout(hidden_layer_1, keep_prob)\n",
    "y_conv = tf.nn.softmax(tf.add(tf.matmul(hidden_layer_1_drop, weights['out']), biases['out']))\n",
    "\n",
    "# Define loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "total_loss = np.zeros(no_of_iterations, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 000/100 cost: 1.269\n",
      "Training accuracy: 0.670\n",
      "Iteration No: 001/100 cost: 1.497\n",
      "Training accuracy: 0.570\n",
      "Iteration No: 002/100 cost: 1.527\n",
      "Training accuracy: 0.520\n",
      "Iteration No: 003/100 cost: 1.327\n",
      "Training accuracy: 0.690\n",
      "Iteration No: 004/100 cost: 1.437\n",
      "Training accuracy: 0.580\n",
      "Iteration No: 005/100 cost: 1.467\n",
      "Training accuracy: 0.570\n",
      "Iteration No: 006/100 cost: 1.483\n",
      "Training accuracy: 0.560\n",
      "Iteration No: 007/100 cost: 1.421\n",
      "Training accuracy: 0.570\n",
      "Iteration No: 008/100 cost: 1.317\n",
      "Training accuracy: 0.720\n",
      "Iteration No: 009/100 cost: 1.215\n",
      "Training accuracy: 0.820\n",
      "Iteration No: 010/100 cost: 1.059\n",
      "Training accuracy: 0.940\n",
      "Iteration No: 011/100 cost: 1.133\n",
      "Training accuracy: 0.920\n",
      "Iteration No: 012/100 cost: 1.049\n",
      "Training accuracy: 0.970\n",
      "Iteration No: 013/100 cost: 1.163\n",
      "Training accuracy: 0.920\n",
      "Iteration No: 014/100 cost: 1.077\n",
      "Training accuracy: 0.940\n",
      "Iteration No: 015/100 cost: 1.094\n",
      "Training accuracy: 0.940\n",
      "Iteration No: 016/100 cost: 1.068\n",
      "Training accuracy: 0.940\n",
      "Iteration No: 017/100 cost: 1.085\n",
      "Training accuracy: 0.920\n",
      "Iteration No: 018/100 cost: 1.236\n",
      "Training accuracy: 0.890\n",
      "Iteration No: 019/100 cost: 1.071\n",
      "Training accuracy: 0.940\n",
      "Iteration No: 020/100 cost: 1.103\n",
      "Training accuracy: 0.930\n",
      "Iteration No: 021/100 cost: 0.987\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 022/100 cost: 1.088\n",
      "Training accuracy: 0.940\n",
      "Iteration No: 023/100 cost: 1.021\n",
      "Training accuracy: 0.970\n",
      "Iteration No: 024/100 cost: 1.041\n",
      "Training accuracy: 0.980\n",
      "Iteration No: 025/100 cost: 0.992\n",
      "Training accuracy: 0.990\n",
      "Iteration No: 026/100 cost: 1.109\n",
      "Training accuracy: 0.960\n",
      "Iteration No: 027/100 cost: 0.983\n",
      "Training accuracy: 0.970\n",
      "Iteration No: 028/100 cost: 1.026\n",
      "Training accuracy: 0.960\n",
      "Iteration No: 029/100 cost: 1.072\n",
      "Training accuracy: 0.970\n",
      "Iteration No: 030/100 cost: 1.048\n",
      "Training accuracy: 0.940\n",
      "Iteration No: 031/100 cost: 1.029\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 032/100 cost: 0.963\n",
      "Training accuracy: 0.980\n",
      "Iteration No: 033/100 cost: 1.042\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 034/100 cost: 1.067\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 035/100 cost: 1.086\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 036/100 cost: 0.973\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 037/100 cost: 0.987\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 038/100 cost: 0.896\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 039/100 cost: 0.947\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 040/100 cost: 0.926\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 041/100 cost: 0.944\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 042/100 cost: 0.820\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 043/100 cost: 0.825\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 044/100 cost: 0.907\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 045/100 cost: 0.761\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 046/100 cost: 0.842\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 047/100 cost: 0.757\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 048/100 cost: 0.730\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 049/100 cost: 0.802\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 050/100 cost: 0.702\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 051/100 cost: 0.645\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 052/100 cost: 0.680\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 053/100 cost: 0.710\n",
      "Training accuracy: 0.980\n",
      "Iteration No: 054/100 cost: 0.676\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 055/100 cost: 0.672\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 056/100 cost: 0.676\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 057/100 cost: 0.656\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 058/100 cost: 0.665\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 059/100 cost: 0.641\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 060/100 cost: 0.628\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 061/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 062/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 063/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 064/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 065/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 066/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 067/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 068/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 069/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 070/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 071/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 072/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 073/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 074/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 075/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 076/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 077/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 078/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 079/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 080/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 081/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 082/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 083/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 084/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 085/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 086/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 087/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 088/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 089/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 090/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 091/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 092/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 093/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 094/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 095/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 096/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 097/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 098/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Iteration No: 099/100 cost: 0.627\n",
      "Training accuracy: 1.000\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))    \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "n_trainset = train_features.shape[0]\n",
    "\n",
    "# Training cycle\n",
    "for i in range(no_of_iterations):\n",
    "    randidx  = np.random.randint(n_trainset, size=batch_size)\n",
    "    batch_elem = v_train_features[randidx, :]\n",
    "    batch_label = trainlabel[randidx, :] \n",
    "        \n",
    "    # Fit training using batch data\n",
    "    sess.run(optimizer, feed_dict={x: batch_elem, y: batch_label, keep_prob: 0.6})\n",
    "    total_loss[i] += sess.run(cost, feed_dict={x: batch_elem, y: batch_label, keep_prob: 1.0})\n",
    "    print (\"Iteration No: %03d/%03d cost: %.3f\" % (i, no_of_iterations, total_loss[i]))\n",
    "    train_acc = sess.run(accuracy, feed_dict={x: batch_elem, y: batch_label, keep_prob: 1.0})\n",
    "    print (\"Training accuracy: %.3f\" % (train_acc))\n",
    "\n",
    "print (\"Finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the network: 0.944\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEZCAYAAABiu9n+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhaCgkREUlLISEiImkpJEREJK3/Azsqu7vlndSRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a6affd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot chart\n",
    "plt.figure(0)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.title(\"loss vs. iteration\")\n",
    "plt.plot(total_loss)\n",
    "\n",
    "test_acc = sess.run(accuracy, feed_dict={x: v_test_features, y: testlabel, keep_prob: 1.0})\n",
    "print (\"Test accuracy of the network: %.3f\" % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
